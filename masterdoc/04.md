## Exercise 1: Explore the offline data and analytics pipeline using open Delta format and Azure Databricks Delta Live Tables. Stitch streaming and non-streaming data landed earlier to create a combined data product to build a simple Lakehouse. <a name="delta-live-table-pipeline"></a>

Wide World Importers has faced challenges in analyzing disparate data sources in an integrated manner. Previously, various teams were tasked with analyzing customer churn, social media trends, marketing campaigns, and sales forecasts independently. This led to the burden of synthesizing these datasets falling on business analysts and executives to make data-driven decisions. By implementing a Lakehouse, collaboration among teams becomes easier within a unified workspace to process, analyze, and model data.

In this exercise, the goal is to merge two sets of data to derive actionable insights. An Azure Databricks Delta Live Table (DLT) pipeline will be established to construct a simple Lakehouse. The pipeline will enhance the data by applying machine learning models to score it, aiding in better understanding customers and mitigating churn.

The data source for this pipeline is the Bronze layer in ADLS Gen2, populated by the Synapse pipeline in Exercise 1. This layer encompasses campaign data, customer churn data, store transactions data, sales data, and Twitter messages.

### Task 1: Set up Azure Databricks environment <a name="adb-env"></a>

In this task, the Azure Databricks environment will be set up.

1. In the Azure portal, type **Azure Databricks (1)** in the search box and select **Azure Databricks (2)** from the results.

    ![Select Azure Databricks](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/image2102.png?raw=true)

1. On the **Azure Databricks** page, select **databricks<inject key="DeploymentId"></inject>**.

1. In the Azure Databricks resource page, select **Launch Workspace**.

    ![Launch Workspace](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/image2104.png?raw=true)

    *A new web session (tab) will open. Now, set up the Databricks compute ready to serve your workload.*

    >**Note:** If a pop-up appears, select **Close**.

5. Navigate to the left navigation pane and select **Workspace (1)**. Then, Expand the **Workspace (2)** section and choose the **ADB_Initial_Setup (3)** notebook.

    ![](../media/04-04-2024(13).png)

    > **Note: Do not execute this script**. 
    
The provided image is for informational purposes only. Due to time constraints, we will not be running this notebook during the lab session.

### Task 2: Review sentiment analysis model training. <a name="sentiment-model"></a>

In this task, you'll explore the sentiment analysis model training notebook to retrieve the model ID used by the DLT pipeline for further data processing.

*Sentiment Analysis is a branch of Natural Language Processing where text is contextually mined to identify and extract subjective information in the source material to understand whether the underlying sentiment is positive, negative, or neutral.*

1. Navigate to the left navigation pane and select **Workspace (1)**. Then, Expand the **Workspace (2)** section and choose the **02_Twitter_Sentiment_Score_Pred_Custom_ML_Model (3)** notebook.

    ![](../media/04-04-2024(14).png)

    > **Note: Do not execute this script**. 
    
 The provided image is for informational purposes only. Due to time constraints, we will not be running this notebook during the lab session.

*Running this script will generate an ML model ID. This Model ID is used by the Delta Live Pipeline that we will create in the next task to perform ML operations on Twitter data.* 

### Task 3: Create a Delta Live Table pipeline. <a name="dlt-pipeline"></a>

In this task, you will create a Delta Live Table pipeline.

*Delta Live Tables (DLT) make it easy to build and manage reliable data pipelines that deliver high-quality data on Delta Lake. DLT helps data engineering teams simplify ETL development and management with declarative pipeline development, automatic data testing, and deep visibility for monitoring and recovery.*

1. Navigate to **Workflows (1)** from the left navigation pane and select the **Delta Live Tables (2)** tab.

    ![](../media/04-04-2024(15).png)

1. Click on **Create Pipeline**.

    ![](../media/04-04-2024(16).png)

1. In the **Create pipeline** window, enter a name like **Delta Live Table Pipeline in the Pipeline** name box.

    ![create pipeline](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/deltalivepipelines.png?raw=true)

5. To set the **Notebook libraries** property, select the notebook icon (at the right).

    ![](../media/04-04-2024(17).png)

6. In the **Select a notebook** window, choose the **03_Sentiment_Analytics_On_Delta_Live_Tables (1)** notebook and click **Select (2)**.

    >**Note:** Due to time constraints, only the **03_Sentiment_Analytics_On_Delta_Live_Tables** notebook library will be added to the pipeline in the lab session.

    ![Select Notebook](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/imageSelectNotebook.png?raw=true)
 
    >**Note**: (TO BE SKIPPED)
    >
    >Similarly, we can repeat steps 5 and 6 to add the other three notebook libraries. 
    >
    >* 01_campaign_analytics_DLT
    >  
    >* Campaign Powered by Twitter
    >
    >* Retail Sales Data Prep Using Spark DLT

1. Enter **/mnt/delta-files/lakedb/ (1)** in the Storage location box.

8. Enter **lakedb (2)** in the Target schema box.

9. Click on **Create (3)**.

    ![Select Create](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/storageupdatedlocation.png?raw=true)

    *Once you click on Create, the Delta Live Table pipeline will be created with all the notebook libraries added to the pipeline.*

    ![Do not select Start](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/img239.png?raw=true)

    > **Note: Do not click on the "Start" button**, as executing the pipeline will take approximately 30 minutes.

    ![Wating for the job to complete](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/image2317.png?raw=true)

    > **Note:** The subsequent instructions are for informational purposes only. Due to time constraints, the pipeline will not be started in the lab session.

10. After approx 30 mins, this is the view you would have seen. **Observe** the data lineage of Bronze, Silver and a variety of Gold tables.

    ![Medallion Architecture](https://github.com/CloudLabsAI-Azure/Ignite-lab/blob/main/media/image2318.png?raw=true)

This pipeline is based on the medallion architecture, a simple yet powerful design pattern for organizing your Lakehouse.

**Bronze** data is usually raw, unprocessed data from source systems.

**Silver** data is created by cleaning and organizing raw data for further analysis and exploration.

**Gold** data is the finished analytical products â€“ star schema tables for BI applications, engineered features for ML models, and shareable data assets for third parties, Data Mesh architectures, and other downstream consumers.

As you can see in this diagram, all the data is first landed in the Lakehouse. This is where it is further processed into different medallion layers, as discussed earlier.

The Twitter sentiment and campaign data are stitched together to create a combined data product. This product is further consumed in the next exercise for machine learning and business intelligence use cases.

This information can then be piped into Microsoft Purview as a part of an overall view of your Azure data estate. The Lakehouse was designed around simplicity, openness, and collaboration. It is an extremely powerful architecture for addressing the many unique and interesting problems of a modern cloud data stack ready to be leveraged by Wide World Importers.

Congratulations! You have now set up a solid foundation of fully stitched data comprised of campaign data and Twitter data from disparate sources, including some key data transformations.
